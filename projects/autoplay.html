<!DOCTYPE html>
<!-- saved from url=(0023)https://embodiedqa.org/ -->
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords"
    content="AutoPlay, MLLM, UI agents, mobile-use agents, reinforcement learning, computer-use agents, vision-language-action">

  <title>AutoPlay</title>
  <meta name="description"
    content="Building vision systems capable of context based common sense reasoning for semantic placement ---">

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="./static/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="./static/main.2f91a3.css" media="screen,projection">
  <link rel="icon" href="./static/seeing-unseen/imgs/SemanticPlacement.png" type="image/icon type">
</head>

<body data-new-gr-c-s-check-loaded="14.1052.0" data-gr-ext-installed="" style="">

  <div class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <a class="navbar-brand" href="https://ram81.github.io/projects/autoplay.html">AutoPlay</a>
        <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>
      <div class="navbar-collapse collapse" id="navbar-main">
        <ul class="nav navbar-nav">
          <li>
            <a href="#overview">Overview</a>
          </li>
          <li>
            <a href="#people">People</a>
          </li>
          <li>
            <a href="#bibtex">Bibtex</a>
          </li>
          <li>
            <a target="_blank" href="">Code</a>
          </li>
          <li>
            <a target="_blank" href="https://arxiv.org/pdf/2504.00907">Paper</a>
          </li>
        </ul>
        <ul class="nav navbar-nav navbar-right">
          <li>
            <a href="">
              <img style="height:40px;" src="./static/autoplay/apple.png">
            </a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <div class="container" id="overview">
    <div class="page-content">
      <p><br></p>
      <div class="row">
        <div class="col-xs-12">
          <h2>Scaling Synthetic Task Generation for Agents via Exploration</h2>
        </div>

        <div class="col-xs-12" style="margin-top: 3px; color: #666;">
          <a target="_blank" href="http://ram81.github.io/">Ram Ramrakhya*</a>,
          <a target="_blank" href="">Andrew Szot</a>,
          <a target="_blank" href="">Omar Attia</a>,
          <a target="_blank" href="">Yuhao Yang</a>,
          <a target="_blank" href="">Anh Nguyen</a>
          <a target="_blank" href="">Bogdan Mazoure</a>
          <a target="_blank" href="">Zhe Gan</a>
          <a target="_blank" href="">Harsh Agrawal</a>, and
          <a target="_blank" href="">Alexander Toshev</a><br>
        </div>
        <div class="col-xs-12" style="margin-top: 3px; color: #666;">
          <!-- Published at <a target="_blank" href="https://cvpr2022.thecvf.com/">CVPR 2022</a>  -->
          [<a href="#bibtex">Bibtex</a>] [<a href="https://arxiv.org/pdf/2504.00907" target="_blank">PDF</a>]
          <!-- [<a target="_blank" href="https://github.com/Ram81/seeing-unseen">Code</a>] -->
           <p></p>
           <p>*Work done while RR was a intern at Apple</p>
        </div>
      </div>

      <div class="row">
        <br>
        <div class="col-xs-12">
          <div class="row">
            <div class="col-sm-12">
              <img src="./static/autoplay/teaser.jpg">
            </div>
          </div>
          <br>

          <div style="padding-top: 20px;">
            <p style="text-align: justify;"><b>AutoPlay.</b> generates large-scale, diverse and verifiable tasks for
              scaling supervision for MLLM agents. In stage 1 (top), \autoplay covers the environment states through a
              MLLM exploration policy that tracks seen states via a memory module. Next, stage 2 (bottom) uses these
              exploratory trajectories and task guideline prompts as context for proposing tasks. The guidelines help
              enforce task diversity and the exploration trajectories uncover environment features and content relevant
              for proposing tasks.</p>
          </div>
        </div>
        <br>

        <div class="col-xs-12" style="text-align: center;">
          <h2>Abstract</h2>
        </div>

        <div class="col-xs-12">
          <!-- <br> -->
          <p>
            Post-Training Multimodal Large Language Models (MLLMs) to build interactive agents holds promise across
            domains such as computer-use, web navigation, and robotics. A key challenge in scaling such post-training is
            lack of high-quality downstream agentic task datasets with tasks that are diverse, feasible, and verifiable.
            Existing approaches for task generation rely heavily on human annotation or prompting MLLM with limited
            downstream environment information, which is either costly or poorly scalable as it yield tasks with limited
            coverage. To remedy this, we present AutoPlay, a scalable pipeline for task generation that explicitly
            explores interactive environments to discover \emph{possible interactions} and \emph{current state}
            information to synthesize environment-grounded tasks. AutoPlay operates in two stages: (i) an exploration
            phase, where an MLLM explorer agent systematically uncovers novel environment states and functionalities,
            and (ii) a task generation phase, where a task generator leverages exploration trajectories and a set of
            task guideline prompts as context to synthesize diverse, executable, and verifiable tasks. We show AutoPlay
            generates 20k tasks across 20 Android applications and 10k tasks across 13 Ubuntu applications to train
            mobile-use and computer-use agents. AutoPlay generated tasks enable large-scale task demonstration synthesis
            without human annotation by employing an MLLM task executor and verifier. This data enables training
            MLLM-based UI agents that improve success rates up to 20.0% on mobile-use and 10.9% on computer-use
            scenarios. In addition, AutoPlay generated tasks combined with MLLM verifier-based rewards enable scaling
            reinforcement learning training of UI agents, leading to an additional 5.7% gain.
            coverage. These results establish AutoPlay as a scalable approach for post-training capable MLLM agents
            reducing reliance on human annotation.
          </p>
          <p>Read more in the <a href="" target="_blank">paper</a>.</p>
        </div>

      </div>
      <br />

      <br />
      <br />

      <div class="row">
        <div class="col-xs-12">
          <h2>Approach</h2>
          <hr />
        </div>

        <div style="padding-top: 20px;">
          <p>We introduce AutoPlay an approach for scalable task generation with a focus on task coverage, feasibility,
            and verifiability. Our method, depicted in Fig.1, incorporates two phases of exploration and task
            generation. First, in *Environment Exploration* phase, an MLLM explorer agent equipped with memory is
            prompted to exhaustively explore an increasing number of novel environment states (top of Fig. 1). Such
            exploratory trajectories are intended to discover the accessible functionalities and content of the
            environment. Next, in *Task Generation* phase, a task generator MLLM uses exploration trajectories as
            environment context to produce diverse environment-grounded tasks based on a set of task guideline prompts
            which describe desired task properties (bottom of Fig. 1). For instance, a task guideline prompt for
            Feature-Use tasks would encourage generation of tasks that require doing diverse create, edit, of delete
            operations on entities in the environment. We present an example of the exploration trajectory collected by
            AutoPlay and the corresponding tasks synthesized using task generator grounded in the state of the
            environment in figure below.</p>
        </div>

        <div class="row">
          <div class="col-sm-12">
            <img src="./static/autoplay/example.jpg">
          </div>
        </div>
        <br>



        <div class="col-xs-12">
          <h2>Qualitative Examples</h2>
          <hr />
        </div>
        <div class="row">
          <div class="col-xs-12" style="margin-bottom: 20px; text-align:center;">
            <label for="appDropdown"><b>Select App:</b></label>
            <select id="appDropdown" class="form-control" style="width: 320px; display: inline-block; color: #000;">
            </select>
          </div>
          <div id="videoContainer">
          </div>

            <style>
            .task-card {
              display: inline-block;
              margin: 5px 3px 0 3px;
              padding: 4px 10px;
              border-radius: 12px;
              font-size: 13px;
              color: #333;
              background: #e3f2fd; /* default pastel blue */
              box-shadow: 0 1px 3px rgba(0,0,0,0.08);
              transition: background 0.3s;
            }
            .task-card.color1 { background: #b3e5fc; } /* pastel blue */
            .task-card.color2 { background: #c8e6c9; } /* pastel green */
            .task-card.color3 { background: #ffcdd2; } /* pastel red */
            .task-card.color4 { background: #bbdefb; } /* muted blue */
            .task-card.color5 { background: #f8bbd0; } /* muted pink/red */
            </style>
          <script>
            let appData = {};
            fetch('./static/autoplay/visuals_by_app.json')
              .then(response => response.json())
              .then(data => {
                const dropdown = document.getElementById('appDropdown');
                dropdown.innerHTML = '';
                Object.keys(data).forEach(app => {
                  const option = document.createElement('option');
                  option.value = app;
                  option.textContent = app.charAt(0).toUpperCase() + app.slice(1);
                  dropdown.appendChild(option);
                });

                appData = data;
                showVideos(document.getElementById('appDropdown').value);
              })
              .catch(error => {
                console.error('Error loading tasks_by_app.json:', error);
              });

            function showVideos(app) {
              const videos = appData[app]["videos"] || [];
              const tasks = appData[app]["tasks"] || [];
              const container = document.getElementById('videoContainer');
              container.innerHTML = videos.map((src, idx) => {
                let taskCards = '';
                console.log(app, tasks[idx]);
                if (tasks[idx] && Array.isArray(tasks[idx])) {
                  taskCards = `<div class="row" style="margin-top:8px; margin-bottom:10px; justify-content:center;">` +
                    tasks[idx].map((task, i) =>
                      `<span class="task-card color${(i%5)+1}">${task}</span>`
                    ).join('') +
                    `</div>`;
                }
                return `<div class="col-md-1"></div>
                  <div class="col-md-3" style="text-align: center;">
                    <center><img src="${src}" style="border: 3px solid #000000;"></center>
                    ${taskCards}
                  </div>`;
              }).join('');
            }

            document.getElementById('appDropdown').addEventListener('change', function () {
              showVideos(this.value);
            });
            // Show default videos on load
            showVideos(document.getElementById('appDropdown').value);
          </script>

        </div>
        <br>

      </div>
      <br />
      <br />

      <!-- <div class="row">
        <div class="col-xs-12">
          <h2>Results</h2>
          <hr />
        </div>

        <div style="padding-top: 0px;">
          <p>We show evaluation results of our approach LLaVA-OV RL against zero-shot baselines leveraging GPT-4o with tool use and with SoM + ReAct prompting, and LLaVA-OV model trained using synthetically generated SFT data.
            Our method (row 6) outperforms all baselines by a significant margin, on success rate by +41.6% on Unseen scenes split and by +31.1% on Unseen tasks split.
          </p>
        </div>

        <div class="row">
          <div class="col-sm-2"></div>
          <div class="col-sm-8">
            <img src="./static/ask-to-act/results.png">
          </div>
        </div>
        <br>

        <div style="padding-top: 0px;">
          <p style="text-align: center;">Tab 1. Evaluation of all methods on Unseen Scenes and Unseen Tasks evaluation splits of Ask-To-Act task. FS
            denotes few-shot examples, * denotes access to privileged information, Full Obs. stands for full observability.</p>
        </div>
      </div>
      <br />
      <br />

      <div class="row">
        <div class="col-xs-12">
          <h2>Analysis</h2>
          <hr />
        </div>
        <div class="row">
          <div class="col-sm-12">
            <img src="./static/ask-to-act/analysis_2.jpeg">
          </div>
        </div>
        <br>

        <div style="padding-top: 0px;">
          <p style="text-align: center;"> Fig. 3 <b>Task Performance vs. Budget of Questions</b>. Evaluation
            performance of policies trained under different budget of questions
            vs. task Success Rate and Ambiguity Resolution Efficiency score.
          </p>
        </div>

        <div style="padding-top: 0px;">
          <p> A desired skill
            for an embodied agent that can ask questions is to adhere
            to user's preferences about how often they would like a
            robot to ask clarification questions. Some would prefer
            an agent ask as few questions as possible for better user. In contrast,
            some users would be fine with an agent asking as many
            questions as it would like to ensure task success rates are
            higher. Motivated by this, we train multiple MLLM policies
            using RL with LLM generated rewards with a variable upperbound on maximum number of questions an agent can ask.
            Specifically, we train policies with a budget of B ∈ {K, K +
            1, K + 2, K + 4} questions, where K is defined as minimum
            required question for a task in Ask-To-Act dataset. In this
            setting, an agent can ask at most B questions in a single
            episode (either relevant or irrelevant) without incurring any
            penalties. Note, for this experiment B is either equal to K
            i.e. ask as close to minimum required questions as possible
            or can be quite high K + 4 where an agent can ask as many
            as 4 extra questions than minimum required in each episode
            without incurring any penalties. Additionally, the agent will
            only be rewarded for relevant questions from all questions it
            asked. We only penalize the agent for each question asked
            after exceeding the question budget B. Fig. 3 shows success
            rates of various policies trained with different budgets under
            the reward setting described in Eq. (1). As shown in Fig. 3
            (a.), as we increase the number of questions the agent can
            ask, the success rates increase; however, there is a clear tradeoff between increase in success rates and question ratio (i.e.
            asked questions to minimum required), see Fig. 3 (b.).</p>
        </div>
      </div>
      <br />
      <br /> -->


      <!-- <div class="row">
        <div class="col-xs-12">
          <h2>Qualitative Examples</h2>
          <hr />
        </div>
        <div class="row">
          <div class="col-sm-12">
            <img src="./static/ask-to-act/qualitative_1.jpeg">
          </div>
        </div>
        <br>

        <div style="padding-top: 0px;">
          <p style="text-align: center;"> Qualitative example of a successful trajectory of our method on an evaluation episode from Unseen Tasks split.</p>
        </div>
      </div>
      <br />
      <br /> -->


      <div class="row" id="bibtex">
        <div class="col-xs-12">
          <h2>Paper</h2>
          <hr>
          <br />
        </div>
        <div class="col-xs-2">
          <center><img style="height: 120px;width: 100px;" src="./static/habitat-web/imgs/habitat-web-thumbnail.png">
          </center>

          <div style="padding-top: 20px;">
            <p style="text-align: center;"><a target="_blank" href="">[Paper]</a></p>
          </div>
        </div>
        <div class="col-xs-10">
          <div class="paper">
            <pre class="paper">
<!--         -->@misc{,
<!--         -->  title={Scaling Synthetic Task Generation for Agents via Exploration},
<!--         -->  author={Ram Ramrakhya and Andrew Szot and Omar Attia and Yuhao Yang and Anh Nguyen and Bogdan Mazoure and Zhe Gan and Harsh Agrawal and Alexander Toshev},
<!--         -->  year={2025},
<!--         -->  eprint={},
<!--         -->  archivePrefix={arXiv},
<!--         -->  primaryClass={cs.AI},
<!--         -->  url={},
<!--         -->}
            </pre>
          </div>
        </div>
      </div>
      <br />
      <br />

      <div class="row" id="people">
        <div class="col-xs-12">
          <h2>People</h2>
          <hr>
          <br />
        </div>
        <!-- <div class="col-md-2 col-sm-3 col-xs-6"></div> -->
        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://ram81.github.io/">
            <img class="people-pic" src="https://ram81.github.io/img/cover.jpg">
          </a>
          <div class="people-name">
            <a href="https://ram81.github.io/">Ram Ramrakhya*</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>
        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://www.andrewszot.com/">
            <img class="people-pic" src="https://www.andrewszot.com/landing/me.jpg">
          </a>
          <div class="people-name">
            <a href="https://www.andrewszot.com/">Andrew Szot</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>

        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://scholar.google.com/citations?user=svCZUyAAAAAJ&hl=en">
            <img class="people-pic" src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=svCZUyAAAAAJ&citpid=1">
          </a>
          <div class="people-name">
            <a href="https://scholar.google.com/citations?user=svCZUyAAAAAJ&hl=en">Omar Attia</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>

        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://yuh-yang.github.io/">
            <img class="people-pic" src="https://yuh-yang.github.io/img/selfie.jpg">
          </a>
          <div class="people-name">
            <a href="https://yuh-yang.github.io/">Yuhao Yang</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>

        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://scholar.google.com/citations?user=r8RQcFQAAAAJ&hl=en">
            <img class="people-pic" src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=r8RQcFQAAAAJ&citpid=1">
          </a>
          <div class="people-name">
            <a href="https://scholar.google.com/citations?user=r8RQcFQAAAAJ&hl=en">Anh Nguyen</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>
        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://bmazoure.github.io/">
            <img class="people-pic" src="https://bmazoure.github.io/assets/img/avatar.jpg">
          </a>
          <div class="people-name">
            <a href="https://bmazoure.github.io/">Bogdan Mazoure</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>
        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://zhegan27.github.io/">
            <img class="people-pic" src="https://zhegan27.github.io/images/Zhe_new2.jpeg">
          </a>
          <div class="people-name">
            <a href="https://zhegan27.github.io/">Zhe Gan</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>
        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://dexter1691.github.io/">
            <img class="people-pic" src="https://dexter1691.github.io/_astro/DP.DW3WkKG5.jpg">
          </a>
          <div class="people-name">
            <a href="https://dexter1691.github.io/">Harsh Agrawal</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>
        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://sites.google.com/view/alextoshev">
            <img class="people-pic"
              src="https://lh3.googleusercontent.com/sitesv/AICyYdZAWimPEX44t7vXRRar2AP-fiKNa507cR0R87qQsNd2QaPOBwKanp_0ds9vAbRgX2o9fbhUnf2nBx6vsWYe8u9rfSCas_70s0swqAJJgr6NLkd9rNMI1xOKcn879hqLrnPs-L8C7E4o62Ffb0dx2i0X1RkzdAlO0uEHGYDGxkpQySUcGIXHmZwRlFhqncR57NcN6j97khe4T5g=w1280">
          </a>
          <div class="people-name">
            <a href="https://sites.google.com/view/alextoshev">Alexander Toshev</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>

      </div>
    </div>
    <br />
    <br />


  </div>
  </div>
  <br />

  <script type="text/javascript" src="./static/jquery.min.js"></script>
  <script type="text/javascript" src="./static/bootstrap.min.js"></script>
</body>

</html>