<!DOCTYPE html>
<!-- saved from url=(0023)https://embodiedqa.org/ -->
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords"
    content="AutoPlay, MLLM, UI agents, mobile-use agents, reinforcement learning, computer-use agents, vision-language-action">

  <title>AutoPlay</title>
  <meta name="description"
    content="Scaling synthetic task generation for Agents via self-play and exploration...">

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="./static/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="./static/main.2f91a3.css" media="screen,projection">
  <link rel="icon" href="./static/autoplay/apple.png" type="image/icon type">
</head>

<body data-new-gr-c-s-check-loaded="14.1052.0" data-gr-ext-installed="" style="">

  <div class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <a class="navbar-brand" href="https://ram81.github.io/projects/autoplay.html">AutoPlay</a>
        <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>
      <div class="navbar-collapse collapse" id="navbar-main">
        <ul class="nav navbar-nav">
          <li>
            <a href="#overview">Overview</a>
          </li>
          <li>
            <a href="#people">People</a>
          </li>
          <li>
            <a href="#bibtex">Bibtex</a>
          </li>
          <li>
            <a target="_blank" href="">Code</a>
          </li>
          <li>
            <a target="_blank" href="">Paper</a>
          </li>
        </ul>
        <ul class="nav navbar-nav navbar-right">
          <li>
            <a href="">
              <img style="height:40px;" src="./static/autoplay/apple.png">
            </a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <div class="container" id="overview">
    <div class="page-content">
      <p><br></p>
      <div class="row">
        <div class="col-xs-12">
          <h2>Scaling Synthetic Task Generation for Agents via Exploration</h2>
        </div>

        <div class="col-xs-12" style="margin-top: 3px; color: #666;">
          <a target="_blank" href="http://ram81.github.io/">Ram Ramrakhya*</a>,
          <a target="_blank" href="https://www.andrewszot.com/">Andrew Szot</a>,
          <a target="_blank" href="https://scholar.google.com/citations?user=svCZUyAAAAAJ&hl=en">Omar Attia</a>,
          <a target="_blank" href="https://yuh-yang.github.io/">Yuhao Yang</a>,
          <a target="_blank" href="https://scholar.google.com/citations?user=r8RQcFQAAAAJ&hl=en">Anh Nguyen</a>
          <a target="_blank" href="https://bmazoure.github.io/">Bogdan Mazoure</a>
          <a target="_blank" href="https://zhegan27.github.io/">Zhe Gan</a>
          <a target="_blank" href="https://dexter1691.github.io/">Harsh Agrawal</a>, and
          <a target="_blank" href="https://sites.google.com/view/alextoshev">Alexander Toshev</a><br>
        </div>
        <div class="col-xs-12" style="margin-top: 3px; color: #666;">
          <!-- Published at <a target="_blank" href="https://cvpr2022.thecvf.com/">CVPR 2022</a>  -->
          [<a href="#bibtex">Bibtex</a>] [<a href="https://arxiv.org/pdf/2504.00907" target="_blank">PDF</a>]
          <!-- [<a target="_blank" href="https://github.com/Ram81/seeing-unseen">Code</a>] -->
           <p></p>
           <p>*Work done while RR was a intern at Apple</p>
        </div>
      </div>

      <div class="row">
        <br>
        <div class="col-xs-12">
          <div class="row">
            <div class="col-sm-12">
              <img src="./static/autoplay/teaser.jpg">
            </div>
          </div>
          <br>

          <div style="padding-top: 20px;">
            <p style="text-align: justify;"><b>AutoPlay</b> generates large-scale, diverse and verifiable tasks for
              scaling supervision for MLLM agents. In stage 1 (top), AutoPlay covers the environment states through a
              MLLM exploration policy that tracks seen states via a memory module. Next, stage 2 (bottom) uses these
              exploratory trajectories and task guideline prompts as context for proposing tasks. The guidelines help
              enforce task diversity and the exploration trajectories uncover environment features and content relevant
              for proposing tasks.</p>
          </div>
        </div>
        <br>

        <div class="col-xs-12" style="text-align: center;">
          <h2>Abstract</h2>
        </div>

        <div class="col-xs-12">
          <!-- <br> -->
          <p>
            Post-Training Multimodal Large Language Models (MLLMs) to build interactive agents holds promise across
            domains such as computer-use, web navigation, and robotics. A key challenge in scaling such post-training is
            lack of high-quality downstream agentic task datasets with tasks that are diverse, feasible, and verifiable.
            Existing approaches for task generation rely heavily on human annotation or prompting MLLM with limited
            downstream environment information, which is either costly or poorly scalable as it yield tasks with limited
            coverage. To remedy this, we present AutoPlay, a scalable pipeline for task generation that explicitly
            explores interactive environments to discover \emph{possible interactions} and \emph{current state}
            information to synthesize environment-grounded tasks. AutoPlay operates in two stages: (i) an exploration
            phase, where an MLLM explorer agent systematically uncovers novel environment states and functionalities,
            and (ii) a task generation phase, where a task generator leverages exploration trajectories and a set of
            task guideline prompts as context to synthesize diverse, executable, and verifiable tasks. We show AutoPlay
            generates 20k tasks across 20 Android applications and 10k tasks across 13 Ubuntu applications to train
            mobile-use and computer-use agents. AutoPlay generated tasks enable large-scale task demonstration synthesis
            without human annotation by employing an MLLM task executor and verifier. This data enables training
            MLLM-based UI agents that improve success rates up to 20.0% on mobile-use and 10.9% on computer-use
            scenarios. In addition, AutoPlay generated tasks combined with MLLM verifier-based rewards enable scaling
            reinforcement learning training of UI agents, leading to an additional 5.7% gain.
            coverage. These results establish AutoPlay as a scalable approach for post-training capable MLLM agents
            reducing reliance on human annotation.
          </p>
          <p>Read more in the <a href="" target="_blank">paper</a>.</p>
        </div>

      </div>
      <br />

      <br />
      <br />

      <div class="row">
        <div class="col-xs-12">
          <h2>Approach</h2>
          <hr />

          <div style="padding-top: 20px;">
            <p>We introduce AutoPlay an approach for scalable task generation with a focus on task coverage, feasibility, and verifiability. Our method, depicted in Fig.1, incorporates two phases of exploration and task generation. First, in <b>Environment Exploration</b> phase, an MLLM explorer agent equipped with memory is prompted to exhaustively explore an increasing number of novel environment states (top of Fig. 1). Such exploratory trajectories are intended to discover the accessible functionalities and content of the environment. Next, in <b>Task Generation</b> phase, a task generator MLLM uses exploration trajectories as environment context to produce diverse environment-grounded tasks based on a set of task guideline prompts which describe desired task properties (bottom of Fig. 1). For instance, a task guideline prompt for Feature-Use tasks would encourage generation of tasks that require doing diverse create, edit, of delete operations on entities in the environment. We present an example of the exploration trajectory collected by AutoPlay and the corresponding tasks synthesized using task generator grounded in the state of the environment in Fig. 2 below.</p>
          </div>

          <div class="row">
            <div class="col-sm-12">
              <img src="./static/autoplay/example.jpg">
            </div>
          </div>
          <br>
          <div style="padding-top: 0px;">
            <p style="text-align: justify;"><b>Fig 2. Task Example</b> Example of task generation based on an environment context, represented as a set of screenshots and interactions, and a set of task guidelines. AutoPlay uses presences of events in the calendar together with guidance of using entities in the context, such as names and dates, to produce Task 1. Similarly, showing the event creation form in the context, coupled with guidance to use these fields as task parameters, results in Task 2.</p>
          </div>
          <br/>

        </div>


        <div class="col-xs-12" id="examples">
          <h2>Exploration Trajectories and Task Examples</h2>
          <hr />
          <div class="row">
            <div class="col-xs-12" style="margin-bottom: 20px; text-align:center;">
              <label for="appDropdown"><b>Select App:</b></label>
              <select id="appDropdown" class="form-control" style="width: 320px; display: inline-block; color: #000;">
              </select>
            </div>
            
            <div id="videoContainer">
            </div>


            <div style="padding-top: 0px;">
              <p style="text-align: justify;"><b>Fig 3. Qualitative Examples</b> of three exploration trajectories gathered during <b>Exploration Phase</b>. Each exploration trajectory provide coverage over novel environment states and functionalities of the application, as illustrated in the demonstrations. Next, using observations from each exploration trajectory as environment context combined with task guideline prompts, the MLLM task generator synthesizes diverse and feasible tasks for each trajectory during <b>Task Generation</b> phase. We present three sample tasks generated for each trajectory below the corresponding demonstration.</p>
            </div>

              <style>
              .task-card {
                display: inline-block;
                margin: 5px 3px 0 3px;
                padding: 4px 10px;
                border-radius: 12px;
                font-size: 13px;
                color: #333;
                background: #e3f2fd; /* default pastel blue */
                box-shadow: 0 1px 3px rgba(0,0,0,0.08);
                transition: background 0.3s;
              }
              .task-card.color1 { background: #b3e5fc; } /* pastel blue */
              .task-card.color2 { background: #c8e6c9; } /* pastel green */
              .task-card.color3 { background: #ffcdd2; } /* pastel red */
              .task-card.color4 { background: #bbdefb; } /* muted blue */
              .task-card.color5 { background: #f8bbd0; } /* muted pink/red */
              </style>
            <script>
              let appData = {};
              fetch('./static/autoplay/visuals_by_app.json')
                .then(response => response.json())
                .then(data => {
                  const dropdown = document.getElementById('appDropdown');
                  dropdown.innerHTML = '';
                  Object.keys(data).forEach(app => {
                    const option = document.createElement('option');
                    option.value = app;
                    option.textContent = app.charAt(0).toUpperCase() + app.slice(1);
                    dropdown.appendChild(option);
                  });

                  appData = data;
                  showVideos(document.getElementById('appDropdown').value);
                })
                .catch(error => {
                  console.error('Error loading tasks_by_app.json:', error);
                });

              function showVideos(app) {
                const videos = appData[app]["videos"] || [];
                const tasks = appData[app]["tasks"] || [];
                const container = document.getElementById('videoContainer');
                container.innerHTML = videos.map((src, idx) => {
                  let taskCards = '';
                  console.log(app, tasks[idx]);
                  if (tasks[idx] && Array.isArray(tasks[idx])) {
                    taskCards = `<div class="row" style="margin-top:8px; margin-bottom:10px; justify-content:center;">` +
                      tasks[idx].map((task, i) =>
                        `<span class="task-card color${(i%5)+1}">${task}</span>`
                      ).join('') +
                      `</div>`;
                  }
                  return `<div class="col-md-1"></div>
                    <div class="col-md-3" style="text-align: center;">
                      <center><img src="${src}" style="border: 3px solid #000000;"></center>
                      ${taskCards}
                    </div>`;
                }).join('');
              }

              document.getElementById('appDropdown').addEventListener('change', function () {
                showVideos(this.value);
              });
              // Show default videos on load
              showVideos(document.getElementById('appDropdown').value);
            </script>

          </div>

        </div>
        <br>

      </div>
      <br />
      <br />

      <div class="row">
        <div class="col-xs-12">
          <h2>Results</h2>
          <hr />
        </div>

        <div class="col-xs-12">
          <div style="padding-top: 0px;">
            <p>We demonstrate that AutoPlay enables scaling generation of synthetic tasks for UI agents, that are feasible, diverse and grounded in environment state and functionality all without human annotations. The resulting high-quality tasks enable scaling of supervised finetuning (SFT) and reinforcement learning (RL) for post-training MLLMs as capable UI agents. 
            </p>


            <p>
              <b>AutoPlay achieves competitive results with Proprietary Baselines trained on human data:</b> Tab. 1 shows that AutoPlay outperforms strong UI agent models such as UI-TARS-1.5, which was trained on large-scale human-annotated GUI data, by 10.2% at the 7B scale and 13.7% at the 72B scale on AndroidWorld. On OSWorld, AutoPlay-72B surpasses the two-stage training pipeline used in AguVis-72B, which leverages both grounding data and human-annotated GUI navigation data, by 5.0% in success rate. Although UI-TARS-2 230B and Seed-VL-1.5 achieve higher performance on AndroidWorld, they rely on substantially larger mixtures of expert models. Similarly, UI-TARS outperforms AutoPlay on OSWorld, likely due to its use of curated, human-labeled UI data—whereas AutoPlay autonomously explores, proposes tasks, and collects data without human supervision.
            </p>

            <p>
              <b>AutoPlay in conjunction with MLLM task verifier enables scaling RL training:</b> In Tab. 1 we also highlight that it is feasible to perform RL training on the AutoPlay generated tasks in conjunction with the MLLM task verifier. This enables RL training on AutoPlay generated tasks without requiring any human annotation or environment feedback. RL allows training on all AutoPlay tasks, even those the executor is not able to solve, enabling a scalable way to train UI agents. We see a 5.7% gain in AndroidWorld. With RL training, the AutoPlay-3B model performs similarly to the AutoPlay-7B model trained with just SFT (39.9% versus 40.1% success rate).
            </p>
          </div>

          <div class="row">
            <div class="col-sm-12">
              <img src="./static/autoplay/results.png">
            </div>
          </div>
          <br>

          <div style="padding-top: 0px;">
            <p style="text-align: justify;"><b>Tab 1. Evaluation results</b> on UI agent benchmarks. Pass@1 and Pass@5 values for AutoPlay models include in parentheses the change relative to the corresponding base model of the same parameter size.</p>
          </div>
        </div>
      </div>
      <br />
      <br />

      <div class="row">
        <div class="col-xs-12">
          <h2>Analysis</h2>
          <hr />
        
          <div class="row">
            <div class="col-sm-12">
              <img src="./static/autoplay/ablation_1.png">
            </div>
          </div>
          <br>

          <div style="padding-top: 0px;">
            <p style="text-align: justify;"> <b>Tab. 2 Ablations</b>. We compare AutoPlay-7B with two baselines that perform lesser exploration, No Exploration and Iterative Exploration, as well as AutoPlay-7B without using task guidelines.
            </p>
          </div>

          <div style="padding-top: 0px;">
            <p> <b>AutoPlay tasks train better agents:</b>  To quantify the importance of the environment exploration we compare with two alternative task proposal baselines. The first baseline, No Exploration, unlike AutoPlay, performs no exploration of the domain. Instead, it generates tasks solely from static environment context, such as textual descriptions and application starting screenshots. The second baseline, Iterative Exploration incorporates limited exploration by sequentially executing a series of short-horizon subgoals and summarzing them as tasks, without performing broad exploration of the domain. As a result, it is more constrained than AutoPlay. Tab. 2 shows that agents trained with AutoPlay tasks outperform agents trained with tasks from No Exploration and Iterative Exploration. Agents trained with AutoPlay tasks outperform those trained with No Exploration tasks by 9.4% average success rate. AutoPlay tends to cover a broader range of functionalities in the environment compared to No Exploration. While Iterative Exploration does interact with the environment to generate tasks, AutoPlay tasks train agents that are 16.6% more successful. This is because Iterative Exploration synthesizes long horizon trajectories by stitching easier short horizon subgoals to guide exploration. This consequently leads to less diverse and easier tasks. In contrast, through rounds of long-horizon exploration, AutoPlay generates diverse tasks that provide broad coverage over app functionalities.</p>
          </div>

          <br>
          <br>

          <div class="row">
            <div class="col-sm-12">
              <img src="./static/autoplay/task_distribution.png">
            </div>
          </div>
          <br>

          <div style="padding-top: 0px;">
            <p style="text-align: center;"> <b>Fig. 3 Task Coverage</b>. Left: For a set of predefined categories, we compare the task distribution across AndroidWorld/OSWorld test tasks and AutoPlay generated tasks in light color. Additionally, we show number of tasks that AutoPlay-7B solves in the benchmark (in blue) vs the number of tasks that get executed to produce training trajectories (in green) using dark colors. Right: For a set of predefined categories, we compare the task distribution across AutoPlay generated tasks and No Exploration generated tasks in light color. Additionally, we show number of tasks that AutoPlay-7B executor solves for both task sets.
            </p>
          </div>

          <div style="padding-top: 0px;">
            <p> <b>Exploration generates more diverse tasks</b>: We compare the task distributions generated by AutoPlay and No Exploration in Fig. 3. The distribution is computed over manually defined task categories that cover a broad range of possible tasks. For example, tasks in the Composition category require combining multiple skills or subtasks to achieve the overall goal (e.g., "Find when John is free and schedule a meeting with him for this week."). Additional details about the task categories are provided in Section E. Although the two distributions exhibit similar trends—categories that are more prevalent under one method tend to be prevalent under the other—the results consistently show lower execution success rates for No Exploration compared to AutoPlay, particularly in categories such as deleting, editing, or retrieving in-app data. This highlights the importance of grounding task generation in exploration.</p>
          </div>
        </div>
      </div>
      <br />
      <br />


      <div class="row" id="bibtex">
        <div class="col-xs-12">
          <h2>Paper</h2>
          <hr>
          <br />
        </div>
        <div class="col-xs-2">
          <center><img style="height: 120px;width: 100px;" src="./static/habitat-web/imgs/habitat-web-thumbnail.png">
          </center>

          <div style="padding-top: 20px;">
            <p style="text-align: center;"><a target="_blank" href="">[Paper]</a></p>
          </div>
        </div>
        <div class="col-xs-10">
          <div class="paper">
            <pre class="paper">
<!--         -->@misc{,
<!--         -->  title={Scaling Synthetic Task Generation for Agents via Exploration},
<!--         -->  author={Ram Ramrakhya and Andrew Szot and Omar Attia and Yuhao Yang and Anh Nguyen and Bogdan Mazoure and Zhe Gan and Harsh Agrawal and Alexander Toshev},
<!--         -->  year={2025},
<!--         -->  eprint={},
<!--         -->  archivePrefix={arXiv},
<!--         -->  primaryClass={cs.AI},
<!--         -->  url={},
<!--         -->}
            </pre>
          </div>
        </div>
      </div>
      <br />
      <br />

      <div class="row" id="people">
        <div class="col-xs-12">
          <h2>People</h2>
          <hr>
          <br />
        </div>
        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://ram81.github.io/">
            <img class="people-pic" src="https://ram81.github.io/img/cover.jpg">
          </a>
          <div class="people-name">
            <a href="https://ram81.github.io/">Ram Ramrakhya*</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>
        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://www.andrewszot.com/">
            <img class="people-pic" src="https://www.andrewszot.com/landing/me.jpg">
          </a>
          <div class="people-name">
            <a href="https://www.andrewszot.com/">Andrew Szot</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>

        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://scholar.google.com/citations?user=svCZUyAAAAAJ&hl=en">
            <img class="people-pic" src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=svCZUyAAAAAJ&citpid=1">
          </a>
          <div class="people-name">
            <a href="https://scholar.google.com/citations?user=svCZUyAAAAAJ&hl=en">Omar Attia</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>

        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://yuh-yang.github.io/">
            <img class="people-pic" src="https://yuh-yang.github.io/img/selfie.jpg">
          </a>
          <div class="people-name">
            <a href="https://yuh-yang.github.io/">Yuhao Yang</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>

        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://scholar.google.com/citations?user=r8RQcFQAAAAJ&hl=en">
            <img class="people-pic" src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=r8RQcFQAAAAJ&citpid=1">
          </a>
          <div class="people-name">
            <a href="https://scholar.google.com/citations?user=r8RQcFQAAAAJ&hl=en">Anh Nguyen</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>
        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://bmazoure.github.io/">
            <img class="people-pic" src="https://bmazoure.github.io/assets/img/avatar.jpg">
          </a>
          <div class="people-name">
            <a href="https://bmazoure.github.io/">Bogdan Mazoure</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>
        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://zhegan27.github.io/">
            <img class="people-pic" src="https://zhegan27.github.io/images/Zhe_new2.jpeg">
          </a>
          <div class="people-name">
            <a href="https://zhegan27.github.io/">Zhe Gan</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>
        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://dexter1691.github.io/">
            <img class="people-pic" src="https://dexter1691.github.io/_astro/DP.DW3WkKG5.jpg">
          </a>
          <div class="people-name">
            <a href="https://dexter1691.github.io/">Harsh Agrawal</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>
        <div class="col-md-2 col-sm-3 col-xs-6">
          <a href="https://sites.google.com/view/alextoshev">
            <img class="people-pic"
              src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=T6PbwPIAAAAJ&citpid=1">
          </a>
          <div class="people-name">
            <a href="https://sites.google.com/view/alextoshev">Alexander Toshev</a><br>
            <affiliation>Apple</affiliation>
          </div>
        </div>

      </div>
    </div>
    <br />
    <br />


  </div>
  </div>
  <br />

  <script type="text/javascript" src="./static/jquery.min.js"></script>
  <script type="text/javascript" src="./static/bootstrap.min.js"></script>
</body>

</html>